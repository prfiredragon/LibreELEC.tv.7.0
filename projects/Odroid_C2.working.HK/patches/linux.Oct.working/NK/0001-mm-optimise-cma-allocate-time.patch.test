From efd8e79c7f4212dae1a05860653174dc6242a437 Mon Sep 17 00:00:00 2001
From: Jamie Coldhill <wrxtasy@amnet.net.au>
Date: Sun, 2 Oct 2016 21:15:23 +0800
Subject: [PATCH] mm: optimise cma allocate time

---
 drivers/cpufreq/cpufreq_hotplug.c | 116 ++++++++---
 include/linux/mm.h                |  42 ++--
 mm/cma.c                          |  23 ++-
 mm/migrate.c                      |   2 +-
 mm/page_alloc.c                   | 410 ++++++++++++++++++++------------------
 mm/page_isolation.c               |  12 +-
 6 files changed, 347 insertions(+), 258 deletions(-)

diff --git a/drivers/cpufreq/cpufreq_hotplug.c b/drivers/cpufreq/cpufreq_hotplug.c
index 8a4921a..0eb99a3 100644
--- a/drivers/cpufreq/cpufreq_hotplug.c
+++ b/drivers/cpufreq/cpufreq_hotplug.c
@@ -546,25 +546,62 @@ wait_next_event:
 	}
 	return 1;
 }
-void cpufreq_set_max_cpu_num(unsigned int cpu_num)
+
+static int hg_set_max_cpu;
+void cpufreq_set_max_cpu_num(unsigned int cpu_num, int cluster_id)
 {
+	struct cpufreq_policy *policy = NULL;
+	struct dbs_data *dbs_data = NULL;
+	unsigned int is_hg = 0;
+	unsigned int little_cores;
+
+	if (cluster_id)
+		return;
+	/*dev_err(NULL, " %s <:%d %d>\n", __func__, cpu_num , cluster_id);*/
+	policy = per_cpu(hg_cpu_dbs_info, 0).cdbs.cur_policy;
+	if (policy) {
+		dbs_data = policy->governor_data;
+		is_hg = (dbs_data->cdata->governor == GOV_HOTPLUG) ? 1 : 0;
+	}
+
+	little_cores = cpumask_weight(&hmp_slow_cpu_mask);
+	cpu_num += little_cores;
 	if (cpu_num >= num_possible_cpus()) {
-		max_cpu_num = num_possible_cpus();
+		cpu_num = num_possible_cpus();
 	} else {
-		if (cpu_num > last_max_cpu_num)
-			max_cpu_num = cpu_num;
-		else {
-			max_cpu_num = cpu_num;
-			if (cpu_num >= num_online_cpus())
-				return;
-			cpu_hotplug_flag = CPU_HOTPLUG_UNPLUG;
+		if (cpu_num < little_cores + 1)
+			cpu_num = little_cores + 1;
+	}
+
+	max_cpu_num = cpu_num;
+	if (cpu_num > last_max_cpu_num) {
+		if (!is_hg) {
+			cpu_hotplug_flag = CPU_HOTPLUG_PLUG;
+			hg_set_max_cpu = 1;
 			if (cpu_hotplug_task)
 				wake_up_process(cpu_hotplug_task);
 		}
+	} else {
+		if (cpu_num >= num_online_cpus())
+			return;
+		cpu_hotplug_flag = CPU_HOTPLUG_UNPLUG;
+		hg_set_max_cpu = 1;
+		if (cpu_hotplug_task)
+			wake_up_process(cpu_hotplug_task);
 	}
 	last_max_cpu_num = max_cpu_num;
 	return;
 }
+
+static bool can_down(void)
+{
+	bool ret = true;
+#ifdef CONFIG_CMA
+	ret &= cma_alloc_ref() > 0 ? false : true;
+#endif
+	return ret;
+}
+
 static int __ref cpu_hotplug_thread(void *data)
 {
 	int i, j, target_cpu = 1;
@@ -580,25 +617,29 @@ static int __ref cpu_hotplug_thread(void *data)
 	cpu = get_cpu();
 	put_cpu();
 	dbs_info = &per_cpu(hg_cpu_dbs_info, cpu);
+
 	while (1) {
 		if (dbs_info->enable)
 			break;
+		if (hg_set_max_cpu)
+			break;
 		set_current_state(TASK_INTERRUPTIBLE);
 		schedule();
 		set_current_state(TASK_RUNNING);
 	}
 	policy = dbs_info->cdbs.cur_policy;
-	dbs_data = policy->governor_data;
-	hg_tuners = dbs_data->tuners;
-
-	dbs_info = &per_cpu(hg_cpu_dbs_info, policy->cpu);
-
+	if (policy) {
+		dbs_data = policy->governor_data;
+		hg_tuners = dbs_data->tuners;
+		dbs_info = &per_cpu(hg_cpu_dbs_info, policy->cpu);
+	}
 	while (1) {
 		if (kthread_should_stop())
 			break;
 
-		mutex_lock(&dbs_info->hotplug_thread_mutex);
-		if (!dbs_info->enable)
+		if (policy)
+			mutex_lock(&dbs_info->hotplug_thread_mutex);
+		if (!(dbs_info->enable || hg_set_max_cpu))
 			goto wait_next_hotplug;
 		if (*hotplug_flag == CPU_HOTPLUG_PLUG) {
 			*hotplug_flag = CPU_HOTPLUG_NONE;
@@ -608,14 +649,17 @@ static int __ref cpu_hotplug_thread(void *data)
 				if (cpu_online(i))
 					continue;
 				j++;
-				cpu_up(i);
+				device_online(get_cpu_device(i));
 				cpumask_set_cpu(i, tsk_cpus_allowed(NULL_task));
-				if (j >= hg_tuners->cpu_num_plug_once)
-					break;
+				if (policy && !hg_set_max_cpu)
+					if (j >= hg_tuners->cpu_num_plug_once)
+						break;
 			}
 		} else if (*hotplug_flag == CPU_HOTPLUG_UNPLUG) {
 			*hotplug_flag = CPU_HOTPLUG_NONE;
 			cpu_down_num = 0;
+			if (!can_down())
+				goto wait_next_hotplug;
 			for (i = 0; i < num_online_cpus()-1; i++) {
 				raw_spin_lock_irqsave(
 					  &NULL_task->pi_lock, flags);
@@ -624,28 +668,38 @@ static int __ref cpu_hotplug_thread(void *data)
 						 SD_BALANCE_EXEC, 0);
 				raw_spin_unlock_irqrestore
 					(&NULL_task->pi_lock, flags);
+				if (cpumask_test_cpu(target_cpu,
+					&hmp_slow_cpu_mask)) {
+					i--;
+					goto clear_cpu;
+				}
 				if (target_cpu == 0) {
 					i--;
 					goto clear_cpu;
 				}
 				if (!cpu_active(target_cpu))
 					goto clear_cpu;
-				cpu_down(target_cpu);
+				device_offline(get_cpu_device(target_cpu));
 				cpu_down_num++;
 clear_cpu:
 				cpumask_clear_cpu(target_cpu,
 					  tsk_cpus_allowed(NULL_task));
-				if (cpu_down_num
-					>= hg_tuners->cpu_num_unplug_once ||
-				   cpu_down_num
-				   >= hg_tuners->each_cpu_num_unplug_once
-				   ) {
+				if (policy && !hg_set_max_cpu) {
+					if (cpu_down_num >=
+					hg_tuners->cpu_num_unplug_once
+					|| cpu_down_num >=
+					hg_tuners->each_cpu_num_unplug_once
+					   ) {
+						break;
+					}
+				} else if (num_online_cpus() <= max_cpu_num)
 					break;
-				}
 			}
 		}
 wait_next_hotplug:
-		mutex_unlock(&dbs_info->hotplug_thread_mutex);
+		hg_set_max_cpu = 0;
+		if (policy)
+			mutex_unlock(&dbs_info->hotplug_thread_mutex);
 		set_current_state(TASK_INTERRUPTIBLE);
 		schedule();
 		set_current_state(TASK_RUNNING);
@@ -1016,8 +1070,12 @@ static int __init cpufreq_gov_dbs_init(void)
 		return err;
 	}
 
-	for (i = 1; i < num_possible_cpus(); i++)
-		cpumask_set_cpu(i, tsk_cpus_allowed(NULL_task));
+	for (i = 1; i < num_possible_cpus(); i++) {
+		if (cpumask_test_cpu(i, &hmp_slow_cpu_mask))
+			cpumask_clear_cpu(i, tsk_cpus_allowed(NULL_task));
+		else
+			cpumask_set_cpu(i, tsk_cpus_allowed(NULL_task));
+	}
 	cpumask_clear_cpu(0, tsk_cpus_allowed(NULL_task));
 	wake_up_process(NULL_task);
 
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 5004ca3..48bc0fe 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -48,6 +48,17 @@ extern int sysctl_legacy_va_layout;
 #define sysctl_legacy_va_layout 0
 #endif
 
+#ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS
+extern const int mmap_rnd_bits_min;
+extern const int mmap_rnd_bits_max;
+extern int mmap_rnd_bits __read_mostly;
+#endif
+#ifdef CONFIG_HAVE_ARCH_MMAP_RND_COMPAT_BITS
+extern const int mmap_rnd_compat_bits_min;
+extern const int mmap_rnd_compat_bits_max;
+extern int mmap_rnd_compat_bits __read_mostly;
+#endif
+
 #ifdef CONFIG_CMA
 #define MIGRATE_CMA_HOLD  1
 #define MIGRATE_CMA_ALLOC 2
@@ -63,6 +74,9 @@ extern int migrate_status;
 extern int mutex_status;
 extern int migrate_refcount;
 extern wait_queue_head_t migrate_wq;
+extern int cma_alloc_ref(void);
+extern void get_cma_alloc_ref(void);
+extern void put_cma_alloc_ref(void);
 #endif
 #include <asm/page.h>
 #include <asm/pgtable.h>
@@ -1028,7 +1042,6 @@ static inline int page_mapped(struct page *page)
 #define VM_FAULT_WRITE	0x0008	/* Special case for get_user_pages */
 #define VM_FAULT_HWPOISON 0x0010	/* Hit poisoned small page */
 #define VM_FAULT_HWPOISON_LARGE 0x0020  /* Hit poisoned large page. Index encoded in upper bits */
-#define VM_FAULT_SIGSEGV 0x0040
 
 #define VM_FAULT_NOPAGE	0x0100	/* ->fault installed the pte, not return page */
 #define VM_FAULT_LOCKED	0x0200	/* ->fault locked the returned page */
@@ -1037,9 +1050,8 @@ static inline int page_mapped(struct page *page)
 
 #define VM_FAULT_HWPOISON_LARGE_MASK 0xf000 /* encodes hpage index for large hwpoison */
 
-#define VM_FAULT_ERROR	(VM_FAULT_OOM | VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV | \
-			 VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE | \
-			 VM_FAULT_FALLBACK)
+#define VM_FAULT_ERROR	(VM_FAULT_OOM | VM_FAULT_SIGBUS | VM_FAULT_HWPOISON | \
+			 VM_FAULT_FALLBACK | VM_FAULT_HWPOISON_LARGE)
 
 /* Encode hstate index for a hwpoisoned large page */
 #define VM_FAULT_SET_HINDEX(x) ((x) << 12)
@@ -1183,28 +1195,6 @@ static inline int fixup_user_fault(struct task_struct *tsk,
 }
 #endif
 
-extern void vma_do_file_update_time(struct vm_area_struct *, const char[], int);
-extern struct file *vma_do_pr_or_file(struct vm_area_struct *, const char[],
-				      int);
-extern void vma_do_get_file(struct vm_area_struct *, const char[], int);
-extern void vma_do_fput(struct vm_area_struct *, const char[], int);
-
-#define vma_file_update_time(vma)	vma_do_file_update_time(vma, __func__, \
-								__LINE__)
-#define vma_pr_or_file(vma)		vma_do_pr_or_file(vma, __func__, \
-							  __LINE__)
-#define vma_get_file(vma)		vma_do_get_file(vma, __func__, __LINE__)
-#define vma_fput(vma)			vma_do_fput(vma, __func__, __LINE__)
-
-#ifndef CONFIG_MMU
-extern struct file *vmr_do_pr_or_file(struct vm_region *, const char[], int);
-extern void vmr_do_fput(struct vm_region *, const char[], int);
-
-#define vmr_pr_or_file(region)		vmr_do_pr_or_file(region, __func__, \
-							  __LINE__)
-#define vmr_fput(region)		vmr_do_fput(region, __func__, __LINE__)
-#endif /* !CONFIG_MMU */
-
 extern int access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len, int write);
 extern int access_remote_vm(struct mm_struct *mm, unsigned long addr,
 		void *buf, int len, int write);
diff --git a/mm/cma.c b/mm/cma.c
index 8b3755d..21ca7d9 100644
--- a/mm/cma.c
+++ b/mm/cma.c
@@ -46,6 +46,25 @@ static struct cma cma_areas[MAX_CMA_AREAS];
 static unsigned cma_area_count;
 static DEFINE_MUTEX(cma_mutex);
 
+static atomic_t cma_allocate;
+int cma_alloc_ref(void)
+{
+	return atomic_read(&cma_allocate);
+}
+EXPORT_SYMBOL(cma_alloc_ref);
+
+void get_cma_alloc_ref(void)
+{
+	atomic_inc(&cma_allocate);
+}
+EXPORT_SYMBOL(get_cma_alloc_ref);
+
+void put_cma_alloc_ref(void)
+{
+	atomic_dec(&cma_allocate);
+}
+EXPORT_SYMBOL(put_cma_alloc_ref);
+
 phys_addr_t cma_get_base(struct cma *cma)
 {
 	return PFN_PHYS(cma->base_pfn);
@@ -137,6 +156,7 @@ static int __init cma_init_reserved_areas(void)
 		if (ret)
 			return ret;
 	}
+	atomic_set(&cma_allocate, 0);
 
 	return 0;
 }
@@ -322,6 +342,7 @@ struct page *cma_alloc(struct cma *cma, int count, unsigned int align)
 	if (!count)
 		return NULL;
 
+	get_cma_alloc_ref();
 	mask = cma_bitmap_aligned_mask(cma, align);
 	bitmap_maxno = cma_bitmap_maxno(cma);
 	bitmap_count = cma_bitmap_pages_to_bits(cma, count);
@@ -359,7 +380,7 @@ struct page *cma_alloc(struct cma *cma, int count, unsigned int align)
 		/* try again with a bit different memory target */
 		start = bitmap_no + mask + 1;
 	}
-
+	put_cma_alloc_ref();
 	pr_debug("%s(): returned %p\n", __func__, page);
 	return page;
 }
diff --git a/mm/migrate.c b/mm/migrate.c
index dec39c9..5b2e285 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -945,7 +945,7 @@ uncharge:
 			add_wait_queue(&migrate_wq, &wait);
 			mutex_unlock(&migrate_wait);
 
-			schedule_timeout_interruptible(20);
+			schedule_timeout_interruptible(msecs_to_jiffies(10));
 
 			mutex_lock(&migrate_wait);
 			remove_wait_queue(&migrate_wq, &wait);
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 28baef1..6d85fad 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -221,6 +221,7 @@ int min_free_order_shift = 1;
  */
 int extra_free_kbytes = 0;
 
+
 static unsigned long __meminitdata nr_kernel_pages;
 static unsigned long __meminitdata nr_all_pages;
 static unsigned long __meminitdata dma_reserve;
@@ -432,8 +433,7 @@ static int destroy_compound_page(struct page *page, unsigned long order)
 	return bad;
 }
 
-static inline void prep_zero_page(struct page *page, unsigned int order,
-							gfp_t gfp_flags)
+static inline void prep_zero_page(struct page *page, int order, gfp_t gfp_flags)
 {
 	int i;
 
@@ -477,7 +477,7 @@ static inline void set_page_guard_flag(struct page *page) { }
 static inline void clear_page_guard_flag(struct page *page) { }
 #endif
 
-static inline void set_page_order(struct page *page, unsigned int order)
+static inline void set_page_order(struct page *page, int order)
 {
 	set_page_private(page, order);
 	__SetPageBuddy(page);
@@ -528,31 +528,21 @@ __find_buddy_index(unsigned long page_idx, unsigned int order)
  * For recording page's order, we use page_private(page).
  */
 static inline int page_is_buddy(struct page *page, struct page *buddy,
-							unsigned int order)
+								int order)
 {
 	if (!pfn_valid_within(page_to_pfn(buddy)))
 		return 0;
 
+	if (page_zone_id(page) != page_zone_id(buddy))
+		return 0;
+
 	if (page_is_guard(buddy) && page_order(buddy) == order) {
 		VM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);
-
-		if (page_zone_id(page) != page_zone_id(buddy))
-			return 0;
-
 		return 1;
 	}
 
 	if (PageBuddy(buddy) && page_order(buddy) == order) {
 		VM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);
-
-		/*
-		 * zone check is done late to avoid uselessly
-		 * calculating zone/node ids for pages that could
-		 * never merge.
-		 */
-		if (page_zone_id(page) != page_zone_id(buddy))
-			return 0;
-
 		return 1;
 	}
 	return 0;
@@ -584,7 +574,6 @@ static inline int page_is_buddy(struct page *page, struct page *buddy,
  */
 
 static inline void __free_one_page(struct page *page,
-		unsigned long pfn,
 		struct zone *zone, unsigned int order,
 		int migratetype)
 {
@@ -601,7 +590,7 @@ static inline void __free_one_page(struct page *page,
 
 	VM_BUG_ON(migratetype == -1);
 
-	page_idx = pfn & ((1 << MAX_ORDER) - 1);
+	page_idx = page_to_pfn(page) & ((1 << MAX_ORDER) - 1);
 
 	VM_BUG_ON_PAGE(page_idx & ((1 << order) - 1), page);
 	VM_BUG_ON_PAGE(bad_range(zone, page), page);
@@ -706,12 +695,9 @@ static void free_pcppages_bulk(struct zone *zone, int count,
 	int migratetype = 0;
 	int batch_free = 0;
 	int to_free = count;
-	unsigned long nr_scanned;
 
 	spin_lock(&zone->lock);
-	nr_scanned = zone_page_state(zone, NR_PAGES_SCANNED);
-	if (nr_scanned)
-		__mod_zone_page_state(zone, NR_PAGES_SCANNED, -nr_scanned);
+	zone->pages_scanned = 0;
 
 	while (to_free) {
 		struct page *page;
@@ -743,7 +729,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,
 			list_del(&page->lru);
 			mt = get_freepage_migratetype(page);
 			/* MIGRATE_MOVABLE list may include MIGRATE_RESERVEs */
-			__free_one_page(page, page_to_pfn(page), zone, 0, mt);
+			__free_one_page(page, zone, 0, mt);
 			trace_mm_page_pcpu_drain(page, 0, mt);
 			if (likely(!is_migrate_isolate_page(page))) {
 				__mod_zone_page_state(zone, NR_FREE_PAGES, 1);
@@ -755,20 +741,17 @@ static void free_pcppages_bulk(struct zone *zone, int count,
 	spin_unlock(&zone->lock);
 }
 
-static void free_one_page(struct zone *zone,
-				struct page *page, unsigned long pfn,
-				unsigned int order,
+static void free_one_page(struct zone *zone, struct page *page, int order,
 				int migratetype)
 {
-	unsigned long nr_scanned;
+	int cur_migratetype;
 	spin_lock(&zone->lock);
-	nr_scanned = zone_page_state(zone, NR_PAGES_SCANNED);
-	if (nr_scanned)
-		__mod_zone_page_state(zone, NR_PAGES_SCANNED, -nr_scanned);
+	zone->pages_scanned = 0;
 
-	__free_one_page(page, pfn, zone, order, migratetype);
-	if (unlikely(!is_migrate_isolate(migratetype)))
-		__mod_zone_freepage_state(zone, 1 << order, migratetype);
+	cur_migratetype = get_pageblock_migratetype(page);
+	__free_one_page(page, zone, order, cur_migratetype);
+	if (unlikely(!is_migrate_isolate(cur_migratetype)))
+		__mod_zone_freepage_state(zone, 1 << order, cur_migratetype);
 	spin_unlock(&zone->lock);
 }
 
@@ -803,16 +786,15 @@ static void __free_pages_ok(struct page *page, unsigned int order)
 {
 	unsigned long flags;
 	int migratetype;
-	unsigned long pfn = page_to_pfn(page);
 
 	if (!free_pages_prepare(page, order))
 		return;
 
-	migratetype = get_pfnblock_migratetype(page, pfn);
 	local_irq_save(flags);
 	__count_vm_events(PGFREE, 1 << order);
+	migratetype = get_pageblock_migratetype(page);
 	set_freepage_migratetype(page, migratetype);
-	free_one_page(page_zone(page), page, pfn, order, migratetype);
+	free_one_page(page_zone(page), page, order, migratetype);
 	local_irq_restore(flags);
 }
 
@@ -945,7 +927,7 @@ static inline int check_new_page(struct page *page)
 	return 0;
 }
 
-static int prep_new_page(struct page *page, unsigned int order, gfp_t gfp_flags)
+static int prep_new_page(struct page *page, int order, gfp_t gfp_flags)
 {
 	int i;
 
@@ -970,18 +952,43 @@ static int prep_new_page(struct page *page, unsigned int order, gfp_t gfp_flags)
 	return 0;
 }
 
+#ifdef CONFIG_CMA
+static inline bool cma_page(struct page *page)
+{
+	int migrate_type = 0;
+
+	migrate_type = get_pageblock_migratetype(page);
+	if (is_migrate_cma(migrate_type) ||
+	   is_migrate_isolate(migrate_type)) {
+		return true;
+	}
+	return false;
+}
+#endif
+
+static inline bool is_writeback(gfp_t flags)
+{
+	return flags & __GFP_WRITE;
+}
+
 /*
  * Go through the free lists for the given migratetype and remove
  * the smallest available page from the freelists
  */
 static inline
 struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,
-						int migratetype)
+						int migratetype, gfp_t flags)
 {
 	unsigned int current_order;
 	struct free_area *area;
 	struct page *page;
 
+#ifdef CONFIG_CMA
+	/* write back pages should not use CMA */
+	if (migratetype == MIGRATE_CMA &&
+		(is_writeback(flags) || cma_alloc_ref()))
+		return NULL;
+#endif
 	/* Find a page of the appropriate size in the preferred list */
 	for (current_order = order; current_order < MAX_ORDER; ++current_order) {
 		area = &(zone->free_area[current_order]);
@@ -1123,8 +1130,8 @@ static void change_pageblock_range(struct page *pageblock_page,
  * nor move CMA pages to different free lists. We don't want unmovable pages
  * to be allocated from MIGRATE_CMA areas.
  *
- * Returns the allocation migratetype if free pages were stolen, or the
- * fallback migratetype if it was decided not to steal.
+ * Returns the new migratetype of the pageblock (or the same old migratetype
+ * if it was unchanged).
  */
 static int try_to_steal_freepages(struct zone *zone, struct page *page,
 				  int start_type, int fallback_type)
@@ -1155,10 +1162,12 @@ static int try_to_steal_freepages(struct zone *zone, struct page *page,
 
 		/* Claim the whole block if over half of it is free */
 		if (pages >= (1 << (pageblock_order-1)) ||
-				page_group_by_mobility_disabled)
+				page_group_by_mobility_disabled) {
+
 			set_pageblock_migratetype(page, start_type);
+			return start_type;
+		}
 
-		return start_type;
 	}
 
 	return fallback_type;
@@ -1166,10 +1175,11 @@ static int try_to_steal_freepages(struct zone *zone, struct page *page,
 
 /* Remove an element from the buddy allocator from the fallback list */
 static inline struct page *
-__rmqueue_fallback(struct zone *zone, unsigned int order, int start_migratetype)
+__rmqueue_fallback(struct zone *zone, int order,
+		   int start_migratetype, gfp_t gfp_flag)
 {
 	struct free_area *area;
-	unsigned int current_order;
+	int current_order;
 	struct page *page;
 	int migratetype, new_type, i;
 #ifdef CONFIG_CMA
@@ -1178,9 +1188,8 @@ __rmqueue_fallback(struct zone *zone, unsigned int order, int start_migratetype)
 
 	start_migratetype &= (~__GFP_BDEV);
 	/* Find the largest possible block of pages in the other list */
-	for (current_order = MAX_ORDER-1;
-				current_order >= order && current_order <= MAX_ORDER-1;
-				--current_order) {
+	for (current_order = MAX_ORDER-1; current_order >= order;
+						--current_order) {
 		for (i = 0;; i++) {
 			migratetype = fallbacks[start_migratetype][i];
 
@@ -1188,8 +1197,13 @@ __rmqueue_fallback(struct zone *zone, unsigned int order, int start_migratetype)
 			if (migratetype == MIGRATE_RESERVE)
 				break;
 #ifdef CONFIG_CMA
-			if (flags && migratetype == MIGRATE_CMA)
-				continue;
+			/* write back pages should not use CMA */
+			if (migratetype == MIGRATE_CMA) {
+				if (flags                  ||
+				    is_writeback(gfp_flag) ||
+				    cma_alloc_ref())
+					continue;
+			}
 #endif
 			area = &(zone->free_area[current_order]);
 			if (list_empty(&area->free_list[migratetype]))
@@ -1219,7 +1233,7 @@ __rmqueue_fallback(struct zone *zone, unsigned int order, int start_migratetype)
 			set_freepage_migratetype(page, new_type);
 
 			trace_mm_page_alloc_extfrag(page, order, current_order,
-				start_migratetype, migratetype);
+				start_migratetype, migratetype, new_type);
 
 			return page;
 		}
@@ -1233,7 +1247,7 @@ __rmqueue_fallback(struct zone *zone, unsigned int order, int start_migratetype)
  * Call me with the zone->lock already held.
  */
 static struct page *__rmqueue(struct zone *zone, unsigned int order,
-						int migratetype)
+						int migratetype, gfp_t gfp_flag)
 {
 	struct page *page;
 	int ori_migratetype = migratetype;
@@ -1258,7 +1272,8 @@ static struct page *__rmqueue(struct zone *zone, unsigned int order,
 				break;
 		}
 		if (tmp_migratetype == MIGRATE_CMA) {
-			page = __rmqueue_smallest(zone, order, MIGRATE_CMA);
+			page = __rmqueue_smallest(zone, order,
+						  MIGRATE_CMA, gfp_flag);
 			if (page) {
 				ori_migratetype = MIGRATE_CMA;
 				goto alloc_page_success;
@@ -1267,10 +1282,10 @@ static struct page *__rmqueue(struct zone *zone, unsigned int order,
 	}
 #endif
 retry_reserve:
-	page = __rmqueue_smallest(zone, order, ori_migratetype);
+	page = __rmqueue_smallest(zone, order, ori_migratetype, gfp_flag);
 
 	if (unlikely(!page) && ori_migratetype != MIGRATE_RESERVE) {
-		page = __rmqueue_fallback(zone, order, migratetype);
+		page = __rmqueue_fallback(zone, order, migratetype, gfp_flag);
 
 		/*
 		 * Use MIGRATE_RESERVE rather than fail an allocation. goto
@@ -1296,13 +1311,13 @@ alloc_page_success:
  */
 static int rmqueue_bulk(struct zone *zone, unsigned int order,
 			unsigned long count, struct list_head *list,
-			int migratetype, bool cold)
+			int migratetype, int cold, gfp_t flags)
 {
 	int i;
 
 	spin_lock(&zone->lock);
 	for (i = 0; i < count; ++i) {
-		struct page *page = __rmqueue(zone, order, migratetype);
+		struct page *page = __rmqueue(zone, order, migratetype, flags);
 		if (unlikely(page == NULL))
 			break;
 
@@ -1315,7 +1330,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,
 		 * merge IO requests if the physical pages are ordered
 		 * properly.
 		 */
-		if (likely(!cold))
+		if (likely(cold == 0))
 			list_add(&page->lru, list);
 		else
 			list_add_tail(&page->lru, list);
@@ -1444,7 +1459,7 @@ void mark_free_pages(struct zone *zone)
 {
 	unsigned long pfn, max_zone_pfn;
 	unsigned long flags;
-	unsigned int order, t;
+	int order, t;
 	struct list_head *curr;
 
 	if (zone_is_empty(zone))
@@ -1476,20 +1491,19 @@ void mark_free_pages(struct zone *zone)
 
 /*
  * Free a 0-order page
- * cold == true ? free a cold page : free a hot page
+ * cold == 1 ? free a cold page : free a hot page
  */
-void free_hot_cold_page(struct page *page, bool cold)
+void free_hot_cold_page(struct page *page, int cold)
 {
 	struct zone *zone = page_zone(page);
 	struct per_cpu_pages *pcp;
 	unsigned long flags;
-	unsigned long pfn = page_to_pfn(page);
 	int migratetype;
 
 	if (!free_pages_prepare(page, 0))
 		return;
 
-	migratetype = get_pfnblock_migratetype(page, pfn);
+	migratetype = get_pageblock_migratetype(page);
 	set_freepage_migratetype(page, migratetype);
 	local_irq_save(flags);
 	__count_vm_event(PGFREE);
@@ -1504,17 +1518,17 @@ void free_hot_cold_page(struct page *page, bool cold)
 	if (migratetype >= MIGRATE_PCPTYPES) {
 		if (unlikely(is_migrate_isolate(migratetype))
 			|| unlikely(is_migrate_cma(migratetype))) {
-			free_one_page(zone, page, pfn, 0, migratetype);
+			free_one_page(zone, page, 0, migratetype);
 			goto out;
 		}
 		migratetype = MIGRATE_MOVABLE;
 	}
 
 	pcp = &this_cpu_ptr(zone->pageset)->pcp;
-	if (!cold)
-		list_add(&page->lru, &pcp->lists[migratetype]);
-	else
+	if (cold)
 		list_add_tail(&page->lru, &pcp->lists[migratetype]);
+	else
+		list_add(&page->lru, &pcp->lists[migratetype]);
 	pcp->count++;
 	if (pcp->count >= pcp->high) {
 		unsigned long batch = ACCESS_ONCE(pcp->batch);
@@ -1529,7 +1543,7 @@ out:
 /*
  * Free a list of 0-order pages
  */
-void free_hot_cold_page_list(struct list_head *list, bool cold)
+void free_hot_cold_page_list(struct list_head *list, int cold)
 {
 	struct page *page, *next;
 
@@ -1646,12 +1660,12 @@ int split_free_page(struct page *page, enum migrate_type page_type)
  */
 static inline
 struct page *buffered_rmqueue(struct zone *preferred_zone,
-			struct zone *zone, unsigned int order,
-			gfp_t gfp_flags, int migratetype)
+			struct zone *zone, int order, gfp_t gfp_flags,
+			int migratetype)
 {
 	unsigned long flags;
 	struct page *page, *tmp_page = NULL;
-	bool cold = ((gfp_flags & __GFP_COLD) != 0);
+	int cold = !!(gfp_flags & __GFP_COLD);
 
 again:
 	if (likely(order == 0)) {
@@ -1668,7 +1682,7 @@ again:
 #endif
 			pcp->count += rmqueue_bulk(zone, 0,
 					pcp->batch, list,
-					migratetype, cold);
+					migratetype, cold, gfp_flags);
 #ifdef CONFIG_CMA
 			if (gfp_flags & __GFP_BDEV)
 				migratetype &= (~__GFP_BDEV);
@@ -1682,11 +1696,15 @@ again:
 		else
 			page = list_entry(list->next, struct page, lru);
 #ifdef CONFIG_CMA
-		if (gfp_flags & __GFP_BDEV) {
+		/* pcp pages with cma should not used for writeback */
+		if (gfp_flags & __GFP_BDEV  ||
+		    is_writeback(gfp_flags) ||
+		    cma_alloc_ref()) {
 			if (get_freepage_migratetype(page) == MIGRATE_CMA) {
 				spin_lock(&zone->lock);
 				migratetype |= __GFP_BDEV;
-				page = __rmqueue(zone, order, migratetype);
+				page = __rmqueue(zone, order,
+						 migratetype, gfp_flags);
 				migratetype &= (~__GFP_BDEV);
 				spin_unlock(&zone->lock);
 				if (!page)
@@ -1698,7 +1716,8 @@ again:
 		} else if (migratetype == MIGRATE_MOVABLE) {
 			if (get_freepage_migratetype(page) != MIGRATE_CMA) {
 				spin_lock(&zone->lock);
-				tmp_page = __rmqueue(zone, order, MIGRATE_CMA);
+				tmp_page = __rmqueue(zone, order,
+						     MIGRATE_CMA, gfp_flags);
 				spin_unlock(&zone->lock);
 				if (!tmp_page)
 					goto use_pcp_page;
@@ -1727,7 +1746,7 @@ use_pcp_page:
 			WARN_ON_ONCE(order > 1);
 		}
 		spin_lock_irqsave(&zone->lock, flags);
-		page = __rmqueue(zone, order, migratetype);
+		page = __rmqueue(zone, order, migratetype, gfp_flags);
 		spin_unlock(&zone->lock);
 		if (!page)
 			goto failed;
@@ -1738,9 +1757,6 @@ use_pcp_page:
 alloc_sucess:
 #endif
 	__mod_zone_page_state(zone, NR_ALLOC_BATCH, -(1 << order));
-	if (zone_page_state(zone, NR_ALLOC_BATCH) == 0 &&
-	    !zone_is_fair_depleted(zone))
-		zone_set_flag(zone, ZONE_FAIR_DEPLETED);
 
 	__count_zone_vm_events(PGALLOC, zone, 1 << order);
 	zone_statistics(preferred_zone, zone, gfp_flags);
@@ -1837,12 +1853,12 @@ static inline bool should_fail_alloc_page(gfp_t gfp_mask, unsigned int order)
  * Return true if free pages are above 'mark'. This takes into account the order
  * of the allocation.
  */
-static bool __zone_watermark_ok(struct zone *z, unsigned int order,
-			unsigned long mark, int classzone_idx, int alloc_flags,
-			long free_pages)
+static bool __zone_watermark_ok(struct zone *z, int order, unsigned long mark,
+		      int classzone_idx, int alloc_flags, long free_pages)
 {
 	/* free_pages my go negative - that's OK */
 	long min = mark;
+	long lowmem_reserve = z->lowmem_reserve[classzone_idx];
 	int o;
 	long free_cma = 0;
 
@@ -1857,7 +1873,8 @@ static bool __zone_watermark_ok(struct zone *z, unsigned int order,
 		free_cma = zone_page_state(z, NR_FREE_CMA_PAGES);
 #endif
 
-	if (free_pages - free_cma <= min + z->lowmem_reserve[classzone_idx])
+	free_pages -= free_cma;
+	if (free_pages <= min + lowmem_reserve)
 		return false;
 	for (o = 0; o < order; o++) {
 		/* At the next order, this order's pages become unavailable */
@@ -1876,15 +1893,15 @@ static bool __zone_watermark_ok(struct zone *z, unsigned int order,
 	return true;
 }
 
-bool zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
+bool zone_watermark_ok(struct zone *z, int order, unsigned long mark,
 		      int classzone_idx, int alloc_flags)
 {
 	return __zone_watermark_ok(z, order, mark, classzone_idx, alloc_flags,
 					zone_page_state(z, NR_FREE_PAGES));
 }
 
-bool zone_watermark_ok_safe(struct zone *z, unsigned int order,
-			unsigned long mark, int classzone_idx, int alloc_flags)
+bool zone_watermark_ok_safe(struct zone *z, int order, unsigned long mark,
+		      int classzone_idx, int alloc_flags)
 {
 	long free_pages = zone_page_state(z, NR_FREE_PAGES);
 
@@ -2073,18 +2090,6 @@ static inline void init_zone_allows_reclaim(int nid)
 }
 #endif	/* CONFIG_NUMA */
 
-static void reset_alloc_batches(struct zone *preferred_zone)
-{
-	struct zone *zone = preferred_zone->zone_pgdat->node_zones;
-
-	do {
-		mod_zone_page_state(zone, NR_ALLOC_BATCH,
-			high_wmark_pages(zone) - low_wmark_pages(zone) -
-			atomic_long_read(&zone->vm_stat[NR_ALLOC_BATCH]));
-		zone_clear_flag(zone, ZONE_FAIR_DEPLETED);
-	} while (zone++ != preferred_zone);
-}
-
 /*
  * get_page_from_freelist goes through the zonelist trying to allocate
  * a page.
@@ -2092,22 +2097,18 @@ static void reset_alloc_batches(struct zone *preferred_zone)
 static struct page *
 get_page_from_freelist(gfp_t gfp_mask, nodemask_t *nodemask, unsigned int order,
 		struct zonelist *zonelist, int high_zoneidx, int alloc_flags,
-		struct zone *preferred_zone, int classzone_idx, int migratetype)
+		struct zone *preferred_zone, int migratetype)
 {
 	struct zoneref *z;
 	struct page *page = NULL;
+	int classzone_idx;
 	struct zone *zone;
 	nodemask_t *allowednodes = NULL;/* zonelist_cache approximation */
 	int zlc_active = 0;		/* set if using zonelist_cache */
 	int did_zlc_setup = 0;		/* just call zlc_setup() one time */
-	bool consider_zone_dirty = (alloc_flags & ALLOC_WMARK_LOW) &&
-				(gfp_mask & __GFP_WRITE);
-	int nr_fair_skipped = 0;
-	bool zonelist_rescan;
 
+	classzone_idx = zone_idx(preferred_zone);
 zonelist_scan:
-	zonelist_rescan = false;
-
 	/*
 	 * Scan zonelist, looking for a zone with enough free.
 	 * See also __cpuset_node_allowed_softwall() comment in kernel/cpuset.c.
@@ -2119,10 +2120,12 @@ zonelist_scan:
 		if (IS_ENABLED(CONFIG_NUMA) && zlc_active &&
 			!zlc_zone_worth_trying(zonelist, z, allowednodes))
 				continue;
-		if (cpusets_enabled() &&
-			(alloc_flags & ALLOC_CPUSET) &&
+		if ((alloc_flags & ALLOC_CPUSET) &&
 			!cpuset_zone_allowed_softwall(zone, gfp_mask))
 				continue;
+		BUILD_BUG_ON(ALLOC_NO_WATERMARKS < NR_WMARK);
+		if (unlikely(alloc_flags & ALLOC_NO_WATERMARKS))
+			goto try_this_zone;
 		/*
 		 * Distribute pages in proportion to the individual
 		 * zone size to ensure fair page aging.  The zone a
@@ -2131,11 +2134,9 @@ zonelist_scan:
 		 */
 		if (alloc_flags & ALLOC_FAIR) {
 			if (!zone_local(preferred_zone, zone))
-				break;
-			if (zone_is_fair_depleted(zone)) {
-				nr_fair_skipped++;
 				continue;
-			}
+			if (atomic_long_read(&zone->vm_stat[NR_ALLOC_BATCH]) <= 0)
+				continue;
 		}
 		/*
 		 * When allocating a page cache page for writing, we
@@ -2163,19 +2164,15 @@ zonelist_scan:
 		 * will require awareness of zones in the
 		 * dirty-throttling and the flusher threads.
 		 */
-		if (consider_zone_dirty && !zone_dirty_ok(zone))
-			continue;
+		if ((alloc_flags & ALLOC_WMARK_LOW) &&
+		    (gfp_mask & __GFP_WRITE) && !zone_dirty_ok(zone))
+			goto this_zone_full;
 
 		mark = zone->watermark[alloc_flags & ALLOC_WMARK_MASK];
 		if (!zone_watermark_ok(zone, order, mark,
 				       classzone_idx, alloc_flags)) {
 			int ret;
 
-			/* Checked here to keep the fast path fast */
-			BUILD_BUG_ON(ALLOC_NO_WATERMARKS < NR_WMARK);
-			if (alloc_flags & ALLOC_NO_WATERMARKS)
-				goto try_this_zone;
-
 			if (IS_ENABLED(CONFIG_NUMA) &&
 					!did_zlc_setup && nr_online_nodes > 1) {
 				/*
@@ -2237,11 +2234,17 @@ try_this_zone:
 		if (page)
 			break;
 this_zone_full:
-		if (IS_ENABLED(CONFIG_NUMA) && zlc_active)
+		if (IS_ENABLED(CONFIG_NUMA))
 			zlc_mark_zone_full(zonelist, z);
 	}
 
-	if (page) {
+	if (unlikely(IS_ENABLED(CONFIG_NUMA) && page == NULL && zlc_active)) {
+		/* Disable zlc cache for second zonelist scan */
+		zlc_active = 0;
+		goto zonelist_scan;
+	}
+
+	if (page)
 		/*
 		 * page->pfmemalloc is set when ALLOC_NO_WATERMARKS was
 		 * necessary to allocate the page. The expectation is
@@ -2250,37 +2253,8 @@ this_zone_full:
 		 * for !PFMEMALLOC purposes.
 		 */
 		page->pfmemalloc = !!(alloc_flags & ALLOC_NO_WATERMARKS);
-		return page;
-	}
-
-	/*
-	 * The first pass makes sure allocations are spread fairly within the
-	 * local node.  However, the local node might have free pages left
-	 * after the fairness batches are exhausted, and remote zones haven't
-	 * even been considered yet.  Try once more without fairness, and
-	 * include remote zones now, before entering the slowpath and waking
-	 * kswapd: prefer spilling to a remote zone over swapping locally.
-	 */
-	if (alloc_flags & ALLOC_FAIR) {
-		alloc_flags &= ~ALLOC_FAIR;
-		if (nr_fair_skipped) {
-			zonelist_rescan = true;
-			reset_alloc_batches(preferred_zone);
-		}
-		if (nr_online_nodes > 1)
-			zonelist_rescan = true;
-	}
-
-	if (unlikely(IS_ENABLED(CONFIG_NUMA) && zlc_active)) {
-		/* Disable zlc cache for second zonelist scan */
-		zlc_active = 0;
-		zonelist_rescan = true;
-	}
-
-	if (zonelist_rescan)
-		goto zonelist_scan;
 
-	return NULL;
+	return page;
 }
 
 /*
@@ -2389,7 +2363,7 @@ static inline struct page *
 __alloc_pages_may_oom(gfp_t gfp_mask, unsigned int order,
 	struct zonelist *zonelist, enum zone_type high_zoneidx,
 	nodemask_t *nodemask, struct zone *preferred_zone,
-	int classzone_idx, int migratetype)
+	int migratetype)
 {
 	struct page *page;
 
@@ -2415,7 +2389,7 @@ __alloc_pages_may_oom(gfp_t gfp_mask, unsigned int order,
 	page = get_page_from_freelist(gfp_mask|__GFP_HARDWALL, nodemask,
 		order, zonelist, high_zoneidx,
 		ALLOC_WMARK_HIGH|ALLOC_CPUSET,
-		preferred_zone, classzone_idx, migratetype);
+		preferred_zone, migratetype);
 	if (page)
 		goto out;
 
@@ -2450,7 +2424,7 @@ static struct page *
 __alloc_pages_direct_compact(gfp_t gfp_mask, unsigned int order,
 	struct zonelist *zonelist, enum zone_type high_zoneidx,
 	nodemask_t *nodemask, int alloc_flags, struct zone *preferred_zone,
-	int classzone_idx, int migratetype, enum migrate_mode mode,
+	int migratetype, enum migrate_mode mode,
 	bool *contended_compaction, bool *deferred_compaction,
 	unsigned long *did_some_progress)
 {
@@ -2478,7 +2452,7 @@ __alloc_pages_direct_compact(gfp_t gfp_mask, unsigned int order,
 		page = get_page_from_freelist(gfp_mask, nodemask,
 				order, zonelist, high_zoneidx,
 				alloc_flags & ~ALLOC_NO_WATERMARKS,
-				preferred_zone, classzone_idx, migratetype);
+				preferred_zone, migratetype);
 		if (page) {
 			preferred_zone->compact_blockskip_flush = false;
 			compaction_defer_reset(preferred_zone, order, true);
@@ -2510,8 +2484,7 @@ static inline struct page *
 __alloc_pages_direct_compact(gfp_t gfp_mask, unsigned int order,
 	struct zonelist *zonelist, enum zone_type high_zoneidx,
 	nodemask_t *nodemask, int alloc_flags, struct zone *preferred_zone,
-	int classzone_idx, int migratetype,
-	enum migrate_mode mode, bool *contended_compaction,
+	int migratetype, enum migrate_mode mode, bool *contended_compaction,
 	bool *deferred_compaction, unsigned long *did_some_progress)
 {
 	return NULL;
@@ -2551,7 +2524,7 @@ static inline struct page *
 __alloc_pages_direct_reclaim(gfp_t gfp_mask, unsigned int order,
 	struct zonelist *zonelist, enum zone_type high_zoneidx,
 	nodemask_t *nodemask, int alloc_flags, struct zone *preferred_zone,
-	int classzone_idx, int migratetype, unsigned long *did_some_progress)
+	int migratetype, unsigned long *did_some_progress)
 {
 	struct page *page = NULL;
 	bool drained = false;
@@ -2569,8 +2542,7 @@ retry:
 	page = get_page_from_freelist(gfp_mask, nodemask, order,
 					zonelist, high_zoneidx,
 					alloc_flags & ~ALLOC_NO_WATERMARKS,
-					preferred_zone, classzone_idx,
-					migratetype);
+					preferred_zone, migratetype);
 
 	/*
 	 * If an allocation failed after direct reclaim, it could be because
@@ -2593,14 +2565,14 @@ static inline struct page *
 __alloc_pages_high_priority(gfp_t gfp_mask, unsigned int order,
 	struct zonelist *zonelist, enum zone_type high_zoneidx,
 	nodemask_t *nodemask, struct zone *preferred_zone,
-	int classzone_idx, int migratetype)
+	int migratetype)
 {
 	struct page *page;
 
 	do {
 		page = get_page_from_freelist(gfp_mask, nodemask, order,
 			zonelist, high_zoneidx, ALLOC_NO_WATERMARKS,
-			preferred_zone, classzone_idx, migratetype);
+			preferred_zone, migratetype);
 
 		if (!page && gfp_mask & __GFP_NOFAIL)
 			wait_iff_congested(preferred_zone, BLK_RW_ASYNC, HZ/50);
@@ -2609,6 +2581,28 @@ __alloc_pages_high_priority(gfp_t gfp_mask, unsigned int order,
 	return page;
 }
 
+static void reset_alloc_batches(struct zonelist *zonelist,
+				enum zone_type high_zoneidx,
+				struct zone *preferred_zone)
+{
+	struct zoneref *z;
+	struct zone *zone;
+
+	for_each_zone_zonelist(zone, z, zonelist, high_zoneidx) {
+		/*
+		 * Only reset the batches of zones that were actually
+		 * considered in the fairness pass, we don't want to
+		 * trash fairness information for zones that are not
+		 * actually part of this zonelist's round-robin cycle.
+		 */
+		if (!zone_local(preferred_zone, zone))
+			continue;
+		mod_zone_page_state(zone, NR_ALLOC_BATCH,
+			high_wmark_pages(zone) - low_wmark_pages(zone) -
+			atomic_long_read(&zone->vm_stat[NR_ALLOC_BATCH]));
+	}
+}
+
 static void wake_all_kswapds(unsigned int order,
 			     struct zonelist *zonelist,
 			     enum zone_type high_zoneidx,
@@ -2679,7 +2673,7 @@ static inline struct page *
 __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
 	struct zonelist *zonelist, enum zone_type high_zoneidx,
 	nodemask_t *nodemask, struct zone *preferred_zone,
-	int classzone_idx, int migratetype)
+	int migratetype)
 {
 	const gfp_t wait = gfp_mask & __GFP_WAIT;
 	struct page *page = NULL;
@@ -2728,19 +2722,15 @@ restart:
 	 * Find the true preferred zone if the allocation is unconstrained by
 	 * cpusets.
 	 */
-	if (!(alloc_flags & ALLOC_CPUSET) && !nodemask) {
-		struct zoneref *preferred_zoneref;
-		preferred_zoneref = first_zones_zonelist(zonelist, high_zoneidx,
-				NULL,
-				&preferred_zone);
-		classzone_idx = zonelist_zone_idx(preferred_zoneref);
-	}
+	if (!(alloc_flags & ALLOC_CPUSET) && !nodemask)
+		first_zones_zonelist(zonelist, high_zoneidx, NULL,
+					&preferred_zone);
 
 rebalance:
 	/* This is the last chance, in general, before the goto nopage. */
 	page = get_page_from_freelist(gfp_mask, nodemask, order, zonelist,
 			high_zoneidx, alloc_flags & ~ALLOC_NO_WATERMARKS,
-			preferred_zone, classzone_idx, migratetype);
+			preferred_zone, migratetype);
 	if (page)
 		goto got_pg;
 
@@ -2755,7 +2745,7 @@ rebalance:
 
 		page = __alloc_pages_high_priority(gfp_mask, order,
 				zonelist, high_zoneidx, nodemask,
-				preferred_zone, classzone_idx, migratetype);
+				preferred_zone, migratetype);
 		if (page) {
 			goto got_pg;
 		}
@@ -2786,8 +2776,7 @@ rebalance:
 	 */
 	page = __alloc_pages_direct_compact(gfp_mask, order, zonelist,
 					high_zoneidx, nodemask, alloc_flags,
-					preferred_zone,
-					classzone_idx, migratetype,
+					preferred_zone, migratetype,
 					migration_mode, &contended_compaction,
 					&deferred_compaction,
 					&did_some_progress);
@@ -2810,8 +2799,7 @@ rebalance:
 					zonelist, high_zoneidx,
 					nodemask,
 					alloc_flags, preferred_zone,
-					classzone_idx, migratetype,
-					&did_some_progress);
+					migratetype, &did_some_progress);
 	if (page)
 		goto got_pg;
 
@@ -2830,7 +2818,7 @@ rebalance:
 			page = __alloc_pages_may_oom(gfp_mask, order,
 					zonelist, high_zoneidx,
 					nodemask, preferred_zone,
-					classzone_idx, migratetype);
+					migratetype);
 			if (page)
 				goto got_pg;
 
@@ -2871,8 +2859,7 @@ rebalance:
 		 */
 		page = __alloc_pages_direct_compact(gfp_mask, order, zonelist,
 					high_zoneidx, nodemask, alloc_flags,
-					preferred_zone,
-					classzone_idx, migratetype,
+					preferred_zone, migratetype,
 					migration_mode, &contended_compaction,
 					&deferred_compaction,
 					&did_some_progress);
@@ -2899,13 +2886,11 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order,
 {
 	enum zone_type high_zoneidx = gfp_zone(gfp_mask);
 	struct zone *preferred_zone;
-	struct zoneref *preferred_zoneref;
 	struct page *page = NULL;
 	int migratetype = allocflags_to_migratetype(gfp_mask);
 	unsigned int cpuset_mems_cookie;
 	int alloc_flags = ALLOC_WMARK_LOW|ALLOC_CPUSET|ALLOC_FAIR;
 	struct mem_cgroup *memcg = NULL;
-	int classzone_idx;
 
 	gfp_mask &= gfp_allowed_mask;
 
@@ -2935,23 +2920,45 @@ retry_cpuset:
 	cpuset_mems_cookie = read_mems_allowed_begin();
 
 	/* The preferred zone is used for statistics later */
-	preferred_zoneref = first_zones_zonelist(zonelist, high_zoneidx,
+	first_zones_zonelist(zonelist, high_zoneidx,
 				nodemask ? : &cpuset_current_mems_allowed,
 				&preferred_zone);
 	if (!preferred_zone)
 		goto out;
-	classzone_idx = zonelist_zone_idx(preferred_zoneref);
 
 #ifdef CONFIG_CMA
 	if (allocflags_to_migratetype(gfp_mask) == MIGRATE_MOVABLE)
 		alloc_flags |= ALLOC_CMA;
 #endif
+retry:
+	if (global_page_state(NR_FREE_PAGES)  -
+		global_page_state(NR_FREE_CMA_PAGES)
+	   < mem_management_thresh)
+		if (!(gfp_mask & __GFP_NO_KSWAPD))
+			wake_all_kswapds(order, zonelist, high_zoneidx,
+							preferred_zone);
 	/* First allocation attempt */
 	page = get_page_from_freelist(gfp_mask|__GFP_HARDWALL, nodemask, order,
 			zonelist, high_zoneidx, alloc_flags,
-			preferred_zone, classzone_idx, migratetype);
+			preferred_zone, migratetype);
 	if (unlikely(!page)) {
 		/*
+		 * The first pass makes sure allocations are spread
+		 * fairly within the local node.  However, the local
+		 * node might have free pages left after the fairness
+		 * batches are exhausted, and remote zones haven't
+		 * even been considered yet.  Try once more without
+		 * fairness, and include remote zones now, before
+		 * entering the slowpath and waking kswapd: prefer
+		 * spilling to a remote zone over swapping locally.
+		 */
+		if (alloc_flags & ALLOC_FAIR) {
+			reset_alloc_batches(zonelist, high_zoneidx,
+					    preferred_zone);
+			alloc_flags &= ~ALLOC_FAIR;
+			goto retry;
+		}
+		/*
 		 * Runtime PM, block IO and its error handling path
 		 * can deadlock because I/O on the device might not
 		 * complete.
@@ -2959,7 +2966,7 @@ retry_cpuset:
 		gfp_mask = memalloc_noio_flags(gfp_mask);
 		page = __alloc_pages_slowpath(gfp_mask, order,
 				zonelist, high_zoneidx, nodemask,
-				preferred_zone, classzone_idx, migratetype);
+				preferred_zone, migratetype);
 	}
 
 	trace_mm_page_alloc(page, order, gfp_mask, migratetype);
@@ -2975,6 +2982,12 @@ out:
 		goto retry_cpuset;
 
 	memcg_kmem_commit_charge(page, memcg, order);
+#ifdef CONFIG_CMA
+	if (page != NULL && is_writeback(gfp_mask) && cma_page(page)) {
+		WARN(1, "write back in cma, order:%d, mask:%x, migrate:%d\n",
+		     order, gfp_mask, get_pageblock_migratetype(page));
+	}
+#endif
 	return page;
 }
 EXPORT_SYMBOL(__alloc_pages_nodemask);
@@ -3009,7 +3022,7 @@ void __free_pages(struct page *page, unsigned int order)
 {
 	if (put_page_testzero(page)) {
 		if (order == 0)
-			free_hot_cold_page(page, false);
+			free_hot_cold_page(page, 0);
 		else
 			__free_pages_ok(page, order);
 	}
@@ -3394,12 +3407,12 @@ void show_free_areas(unsigned int filter)
 			K(zone_page_state(zone, NR_BOUNCE)),
 			K(zone_page_state(zone, NR_FREE_CMA_PAGES)),
 			K(zone_page_state(zone, NR_WRITEBACK_TEMP)),
-			K(zone_page_state(zone, NR_PAGES_SCANNED)),
+			zone->pages_scanned,
 			(!zone_reclaimable(zone) ? "yes" : "no")
 			);
 		printk("lowmem_reserve[]:");
 		for (i = 0; i < MAX_NR_ZONES; i++)
-			printk(" %ld", zone->lowmem_reserve[i]);
+			printk(" %lu", zone->lowmem_reserve[i]);
 		printk("\n");
 	}
 
@@ -4287,7 +4300,7 @@ void __meminit memmap_init_zone(unsigned long size, int nid, unsigned long zone,
 
 static void __meminit zone_init_free_lists(struct zone *zone)
 {
-	unsigned int order, t;
+	int order, t;
 	for_each_migratetype_order(order, t) {
 		INIT_LIST_HEAD(&zone->free_area[order].free_list[t]);
 		zone->free_area[order].nr_free = 0;
@@ -5728,7 +5741,7 @@ static void calculate_totalreserve_pages(void)
 	for_each_online_pgdat(pgdat) {
 		for (i = 0; i < MAX_NR_ZONES; i++) {
 			struct zone *zone = pgdat->node_zones + i;
-			long max = 0;
+			unsigned long max = 0;
 
 			/* Find valid and maximum lowmem_reserve in the zone */
 			for (j = i; j < MAX_NR_ZONES; j++) {
@@ -6221,16 +6234,17 @@ static inline int pfn_to_bitidx(struct zone *zone, unsigned long pfn)
  * @end_bitidx: The last bit of interest
  * returns pageblock_bits flags
  */
-unsigned long get_pfnblock_flags_mask(struct page *page, unsigned long pfn,
+unsigned long get_pageblock_flags_mask(struct page *page,
 					unsigned long end_bitidx,
 					unsigned long mask)
 {
 	struct zone *zone;
 	unsigned long *bitmap;
-	unsigned long bitidx, word_bitidx;
+	unsigned long pfn, bitidx, word_bitidx;
 	unsigned long word;
 
 	zone = page_zone(page);
+	pfn = page_to_pfn(page);
 	bitmap = get_pageblock_bitmap(zone, pfn);
 	bitidx = pfn_to_bitidx(zone, pfn);
 	word_bitidx = bitidx / BITS_PER_LONG;
@@ -6242,25 +6256,25 @@ unsigned long get_pfnblock_flags_mask(struct page *page, unsigned long pfn,
 }
 
 /**
- * set_pfnblock_flags_mask - Set the requested group of flags for a pageblock_nr_pages block of pages
+ * set_pageblock_flags_mask - Set the requested group of flags for a pageblock_nr_pages block of pages
  * @page: The page within the block of interest
  * @start_bitidx: The first bit of interest
  * @end_bitidx: The last bit of interest
  * @flags: The flags to set
  */
-void set_pfnblock_flags_mask(struct page *page, unsigned long flags,
-					unsigned long pfn,
+void set_pageblock_flags_mask(struct page *page, unsigned long flags,
 					unsigned long end_bitidx,
 					unsigned long mask)
 {
 	struct zone *zone;
 	unsigned long *bitmap;
-	unsigned long bitidx, word_bitidx;
+	unsigned long pfn, bitidx, word_bitidx;
 	unsigned long old_word, word;
 
 	BUILD_BUG_ON(NR_PAGEBLOCK_BITS != 4);
 
 	zone = page_zone(page);
+	pfn = page_to_pfn(page);
 	bitmap = get_pageblock_bitmap(zone, pfn);
 	bitidx = pfn_to_bitidx(zone, pfn);
 	word_bitidx = bitidx / BITS_PER_LONG;
@@ -6553,7 +6567,7 @@ try_again:
 
 	/* Make sure the range is really isolated. */
 	if (test_pages_isolated(outer_start, end, false)) {
-		pr_warn("alloc_contig_range test_pages_isolated(%lx, %lx) failed\n",
+		pr_debug("alloc_contig_range test_pages_isolated(%lx, %lx) failed\n",
 		       outer_start, end);
 		try_times++;
 		if (try_times < 10)
@@ -6640,7 +6654,7 @@ __offline_isolated_pages(unsigned long start_pfn, unsigned long end_pfn)
 {
 	struct page *page;
 	struct zone *zone;
-	unsigned int order, i;
+	int order, i;
 	unsigned long pfn;
 	unsigned long flags;
 	/* find the first valid pfn */
@@ -6694,7 +6708,7 @@ bool is_free_buddy_page(struct page *page)
 	struct zone *zone = page_zone(page);
 	unsigned long pfn = page_to_pfn(page);
 	unsigned long flags;
-	unsigned int order;
+	int order;
 
 	spin_lock_irqsave(&zone->lock, flags);
 	for (order = 0; order < MAX_ORDER; order++) {
diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 29ab686..55fcd64 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -219,9 +219,15 @@ __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn,
 			 */
 			pfn++;
 			continue;
-		}
-		else
+		} else {
+			pr_debug("%s, page:%ld not isolate, flag:%lx, ",
+				__func__, pfn, page->flags);
+			pr_debug("page_cnt:%d, isolate:%d, mapcnt:%d\n",
+				page_count(page),
+				get_freepage_migratetype(page),
+				atomic_read(&page->_mapcount));
 			return -EBUSY;
+		}
 	}
 	if (pfn < end_pfn)
 		return 1;
@@ -265,7 +271,7 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,
 			add_wait_queue(&iso_wq, &wait);
 			mutex_unlock(&iso_wait);
 
-			schedule_timeout_interruptible(20);
+			schedule_timeout_interruptible(msecs_to_jiffies(10));
 
 			mutex_lock(&iso_wait);
 			remove_wait_queue(&iso_wq, &wait);
-- 
1.9.1

